{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f07476c9-9044-4192-9174-af70ff8ebc3c",
   "metadata": {},
   "source": [
    "In this notebooks all the content is study by using code provide by supervision document <br>\n",
    "Link : https://supervision.roboflow.com/0.25.0/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b5a45fa-4eee-4268-9780-8c7e8bcca54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ipywidgets --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ef858d3-c9cd-4b92-80eb-f61514534c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config \n",
    "HOME = \"../../..\"\n",
    "MODEL = f\"{HOME}/notebooks/models/yolov8x.pt\"\n",
    "IMAGE_PATH = f\"{HOME}/data/raw/images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7596c94-076f-44ec-9e1d-18d687fb491a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from supervision.assets import download_assets, VideoAssets # to get the sample video \n",
    "import supervision as sv \n",
    "from ultralytics import YOLO\n",
    "import numpy as np \n",
    "import cv2\n",
    "import matplotlib.pyplot as plt # show image use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d996990-2c63-4e0b-afd1-bbb28b7f6dac",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'download_assets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# get sample video \u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m videoName \u001b[38;5;241m=\u001b[39m download_assets(VideoAssets\u001b[38;5;241m.\u001b[39mPEOPLE_WALKING)\n\u001b[0;32m      4\u001b[0m videoName\n",
      "\u001b[1;31mNameError\u001b[0m: name 'download_assets' is not defined"
     ]
    }
   ],
   "source": [
    "# get sample video \n",
    "videoName = download_assets(VideoAssets.PEOPLE_WALKING)\n",
    "\n",
    "videoName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affdb983-61f5-4cdb-a073-3c3d34e9d4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def fileMoving(source, destination) : \n",
    "    return shutil.move(source, destination) # will return destination path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc92925-3ed9-4bd0-af8f-460e139f4628",
   "metadata": {},
   "outputs": [],
   "source": [
    "videoPath = fileMoving(source=f\"./{videoName}\", destination=f\"{HOME}/data/raw/videos/\")\n",
    "\n",
    "videoPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec7f60e-98e4-4416-9a9e-2e62b035d930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model \n",
    "model = YOLO(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a21a265-2d9d-4df5-864a-69dcc6f7649b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get one frame from the video and display it \n",
    "\n",
    "# get frames iterator \n",
    "frames = sv.get_video_frames_generator(videoPath)\n",
    "\n",
    "singleFrame = next(frames)\n",
    "\n",
    "# show image\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(cv2.cvtColor(singleFrame, cv2.COLOR_BGR2RGB))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf434569-fe2d-4bc7-b1fa-fb9aec196b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the size of the picture \n",
    "# height, width, channels\n",
    "singleFrame.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008c0d56-d612-43e5-ae06-e8a2689af8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw a line with using Point \n",
    "start, end = sv.Point(x=0, y=540), sv.Point(x=1920, y=540)\n",
    "\n",
    "line = sv.LineZone(start=start, end=end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e88137a-0d07-4488-891e-e96ca721fa7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw the line and show on image \n",
    "frameWithLine = singleFrame.copy()\n",
    "\n",
    "# draw line using start and end\n",
    "frameWithLine = cv2.line(frameWithLine, (start.x, start.y), \n",
    "                         (end.x, end.y), color=(0, 255, 0), thickness=10)\n",
    "\n",
    "# display \n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(cv2.cvtColor(frameWithLine, cv2.COLOR_BGR2RGB))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a640a2-9653-43e7-ac01-1496e547b8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply bytetrack with triangle annotated\n",
    "\n",
    "tracker = sv.ByteTrack()\n",
    "triAnnotator = sv.TriangleAnnotator(height=20, base=20) # annotated\n",
    "\n",
    "# do detection\n",
    "result = model(singleFrame, classes=[0])[0]\n",
    "\n",
    "# return result including xyxy, confidence, class for each object detect\n",
    "detections = sv.Detections.from_ultralytics(result) \n",
    "\n",
    "detections = tracker.update_with_detections(detections)\n",
    "\n",
    "# annotated with triangle mark\n",
    "annotatedFrame = triAnnotator.annotate(scene=singleFrame.copy(), detections=detections)\n",
    "\n",
    "# display \n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(cv2.cvtColor(annotatedFrame, cv2.COLOR_BGR2RGB))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350be140-a6a2-4777-81b5-d49667ae826c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add label annotated \n",
    "labels = [\n",
    "    f\"#{tracker_id} {confidence:.2f}\"\n",
    "    for confidence, tracker_id in zip(\n",
    "        detections.confidence, detections.tracker_id\n",
    "    )\n",
    "]\n",
    "labelAnnotator = sv.LabelAnnotator(text_position=sv.Position.CENTER)\n",
    "annotatedFrame = labelAnnotator.annotate(scene=annotatedFrame, detections=detections, labels=labels)\n",
    "\n",
    "# display \n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(cv2.cvtColor(annotatedFrame, cv2.COLOR_BGR2RGB))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7867a497-3081-4552-ba64-733be05d09af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 28 persons, 900.8ms\n",
      "Speed: 6.1ms preprocess, 900.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "goin > 0\n",
      "goout > 0\n",
      "\n",
      "0: 384x640 30 persons, 956.4ms\n",
      "Speed: 6.0ms preprocess, 956.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "goin > 0\n",
      "goout > 0\n",
      "\n",
      "0: 384x640 30 persons, 918.3ms\n",
      "Speed: 4.0ms preprocess, 918.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "goin > 0\n",
      "goout > 0\n",
      "\n",
      "0: 384x640 29 persons, 877.9ms\n",
      "Speed: 3.8ms preprocess, 877.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "goin > 0\n",
      "goout > 0\n",
      "\n",
      "0: 384x640 30 persons, 923.8ms\n",
      "Speed: 4.0ms preprocess, 923.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "goin > 0\n",
      "goout > 0\n",
      "\n",
      "0: 384x640 29 persons, 753.0ms\n",
      "Speed: 3.0ms preprocess, 753.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "goin > 0\n",
      "goout > 0\n",
      "\n",
      "0: 384x640 31 persons, 669.2ms\n",
      "Speed: 3.1ms preprocess, 669.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "goin > 0\n",
      "goout > 0\n",
      "\n",
      "0: 384x640 29 persons, 658.0ms\n",
      "Speed: 3.1ms preprocess, 658.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "goin > 0\n",
      "goout > 0\n",
      "\n",
      "0: 384x640 30 persons, 678.2ms\n",
      "Speed: 3.0ms preprocess, 678.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "goin > 0\n",
      "goout > 0\n",
      "\n",
      "0: 384x640 28 persons, 771.2ms\n",
      "Speed: 2.0ms preprocess, 771.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "goin > 0\n",
      "goout > 0\n",
      "\n",
      "0: 384x640 29 persons, 736.9ms\n",
      "Speed: 3.0ms preprocess, 736.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "goin > 0\n",
      "goout > 0\n",
      "\n",
      "0: 384x640 28 persons, 658.6ms\n",
      "Speed: 3.0ms preprocess, 658.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "goin > 0\n",
      "goout > 0\n",
      "\n",
      "0: 384x640 28 persons, 736.5ms\n",
      "Speed: 3.0ms preprocess, 736.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "goin > 0\n",
      "goout > 0\n",
      "\n",
      "0: 384x640 27 persons, 665.4ms\n",
      "Speed: 3.0ms preprocess, 665.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "goin > 0\n",
      "goout > 0\n",
      "\n",
      "0: 384x640 28 persons, 725.0ms\n",
      "Speed: 2.0ms preprocess, 725.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "goin > 0\n",
      "goout > 0\n",
      "\n",
      "0: 384x640 25 persons, 860.2ms\n",
      "Speed: 2.0ms preprocess, 860.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "goin > 0\n",
      "goout > 0\n",
      "\n",
      "0: 384x640 28 persons, 891.2ms\n",
      "Speed: 5.8ms preprocess, 891.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "goin > 0\n",
      "goout > 0\n",
      "\n",
      "0: 384x640 27 persons, 789.9ms\n",
      "Speed: 4.0ms preprocess, 789.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "goin > 0\n",
      "goout > 0\n",
      "\n",
      "0: 384x640 28 persons, 873.7ms\n",
      "Speed: 3.8ms preprocess, 873.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "goin > 1\n",
      "goout > 1\n",
      "\n",
      "0: 384x640 28 persons, 973.4ms\n",
      "Speed: 2.0ms preprocess, 973.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "goin > 1\n",
      "goout > 1\n",
      "\n",
      "0: 384x640 29 persons, 824.9ms\n",
      "Speed: 3.0ms preprocess, 824.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "goin > 1\n",
      "goout > 1\n",
      "\n",
      "0: 384x640 27 persons, 698.2ms\n",
      "Speed: 4.0ms preprocess, 698.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "goin > 1\n",
      "goout > 1\n",
      "\n",
      "0: 384x640 29 persons, 684.7ms\n",
      "Speed: 3.0ms preprocess, 684.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "goin > 1\n",
      "goout > 1\n",
      "\n",
      "0: 384x640 30 persons, 696.7ms\n",
      "Speed: 3.0ms preprocess, 696.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "goin > 1\n",
      "goout > 1\n",
      "\n",
      "0: 384x640 30 persons, 726.4ms\n",
      "Speed: 2.5ms preprocess, 726.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "goin > 1\n",
      "goout > 1\n",
      "\n",
      "0: 384x640 29 persons, 725.3ms\n",
      "Speed: 4.0ms preprocess, 725.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "goin > 1\n",
      "goout > 1\n",
      "\n",
      "0: 384x640 29 persons, 735.6ms\n",
      "Speed: 3.0ms preprocess, 735.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "goin > 1\n",
      "goout > 1\n",
      "\n",
      "0: 384x640 28 persons, 1017.5ms\n",
      "Speed: 55.0ms preprocess, 1017.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "goin > 1\n",
      "goout > 1\n",
      "\n",
      "0: 384x640 26 persons, 856.7ms\n",
      "Speed: 4.1ms preprocess, 856.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "goin > 1\n",
      "goout > 1\n",
      "\n",
      "0: 384x640 25 persons, 730.6ms\n",
      "Speed: 4.0ms preprocess, 730.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "goin > 1\n",
      "goout > 1\n",
      "\n",
      "0: 384x640 25 persons, 811.7ms\n",
      "Speed: 3.0ms preprocess, 811.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "goin > 1\n",
      "goout > 1\n",
      "\n",
      "0: 384x640 25 persons, 828.4ms\n",
      "Speed: 42.0ms preprocess, 828.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "goin > 1\n",
      "goout > 1\n",
      "\n",
      "0: 384x640 29 persons, 665.9ms\n",
      "Speed: 3.0ms preprocess, 665.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "goin > 1\n",
      "goout > 1\n",
      "\n",
      "0: 384x640 29 persons, 735.0ms\n",
      "Speed: 3.0ms preprocess, 735.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "goin > 1\n",
      "goout > 1\n",
      "\n",
      "0: 384x640 29 persons, 787.8ms\n",
      "Speed: 2.0ms preprocess, 787.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "goin > 1\n",
      "goout > 1\n",
      "\n",
      "0: 384x640 28 persons, 785.8ms\n",
      "Speed: 5.0ms preprocess, 785.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "goin > 1\n",
      "goout > 1\n",
      "\n",
      "0: 384x640 28 persons, 781.0ms\n",
      "Speed: 4.0ms preprocess, 781.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "goin > 1\n",
      "goout > 1\n",
      "\n",
      "0: 384x640 27 persons, 761.2ms\n",
      "Speed: 3.0ms preprocess, 761.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "goin > 1\n",
      "goout > 1\n",
      "\n",
      "0: 384x640 27 persons, 758.0ms\n",
      "Speed: 2.0ms preprocess, 758.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "goin > 1\n",
      "goout > 1\n",
      "\n",
      "0: 384x640 28 persons, 739.3ms\n",
      "Speed: 4.0ms preprocess, 739.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "goin > 2\n",
      "goout > 1\n",
      "\n",
      "0: 384x640 28 persons, 747.1ms\n",
      "Speed: 2.0ms preprocess, 747.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "goin > 2\n",
      "goout > 1\n",
      "\n",
      "0: 384x640 28 persons, 753.1ms\n",
      "Speed: 2.0ms preprocess, 753.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "goin > 2\n",
      "goout > 1\n",
      "\n",
      "0: 384x640 27 persons, 711.9ms\n",
      "Speed: 4.0ms preprocess, 711.9ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "goin > 2\n",
      "goout > 1\n",
      "\n",
      "0: 384x640 29 persons, 710.6ms\n",
      "Speed: 3.1ms preprocess, 710.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "goin > 2\n",
      "goout > 1\n",
      "\n",
      "0: 384x640 29 persons, 713.4ms\n",
      "Speed: 3.0ms preprocess, 713.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "goin > 2\n",
      "goout > 1\n",
      "\n",
      "0: 384x640 27 persons, 708.0ms\n",
      "Speed: 2.0ms preprocess, 708.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "goin > 2\n",
      "goout > 1\n",
      "\n",
      "0: 384x640 29 persons, 686.2ms\n",
      "Speed: 2.0ms preprocess, 686.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "goin > 2\n",
      "goout > 1\n",
      "\n",
      "0: 384x640 27 persons, 682.1ms\n",
      "Speed: 4.0ms preprocess, 682.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "goin > 2\n",
      "goout > 1\n",
      "\n",
      "0: 384x640 28 persons, 955.3ms\n",
      "Speed: 4.0ms preprocess, 955.3ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "goin > 2\n",
      "goout > 1\n",
      "\n",
      "0: 384x640 28 persons, 771.4ms\n",
      "Speed: 3.0ms preprocess, 771.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "goin > 2\n",
      "goout > 1\n",
      "\n",
      "0: 384x640 28 persons, 697.4ms\n",
      "Speed: 2.5ms preprocess, 697.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "goin > 2\n",
      "goout > 1\n",
      "\n",
      "0: 384x640 28 persons, 691.5ms\n",
      "Speed: 3.1ms preprocess, 691.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "goin > 2\n",
      "goout > 1\n",
      "\n",
      "0: 384x640 27 persons, 710.1ms\n",
      "Speed: 2.0ms preprocess, 710.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "goin > 2\n",
      "goout > 1\n"
     ]
    }
   ],
   "source": [
    "# combine line, bytetrack and annotator, display using opencv\n",
    "from ultralytics import YOLO\n",
    "import supervision as sv\n",
    "import cv2\n",
    "import numpy as np\n",
    "from supervision import Position\n",
    "\n",
    "# labeling \n",
    "tracker = sv.ByteTrack()\n",
    "# boxAnnotator = sv.TriangleAnnotator(base=20, height=20)\n",
    "boxAnnotator = sv.BoxAnnotator(thickness=1)\n",
    "labelAnnotator = sv.LabelAnnotator(text_position=sv.Position.CENTER)\n",
    "\n",
    "def annotated(frame: np.ndarray, detections: sv.Detections) -> np.ndarray :\n",
    "    \n",
    "    labels = [\n",
    "        f\"#{id} {confidence:.2f}\"\n",
    "        for id, confidence in zip(\n",
    "            detections.tracker_id, detections.confidence\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    annotatedFrame = boxAnnotator.annotate(\n",
    "        scene=frame.copy(),\n",
    "        detections=detections\n",
    "    )\n",
    "    annotatedFrame = labelAnnotator.annotate(\n",
    "        scene=annotatedFrame.copy(),\n",
    "        detections=detections,\n",
    "        labels=labels\n",
    "    )\n",
    "    return annotatedFrame\n",
    "\n",
    "def calculateWindowSize(scale: float, cap: cv2.VideoCapture) -> np.int64 : \n",
    "    oriWidth, oriHeight = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    return int(oriWidth*scale), int(oriHeight*scale)\n",
    "\n",
    "# x, y \n",
    "def textPosition(width: np.int64, height: np.int64) -> (np.int64) : \n",
    "    return (width - 200, height - 100)\n",
    "\n",
    "# path to home\n",
    "HOME = \"../../..\"\n",
    "\n",
    "videoPath = f\"{HOME}/data/raw/videos/people-walking.mp4\"\n",
    "modelPath = f\"{HOME}/notebooks/models/yolov8x.pt\"\n",
    "\n",
    "model = YOLO(modelPath)\n",
    "\n",
    "cap = cv2.VideoCapture(videoPath)\n",
    "\n",
    "# calculate resize value and the center point of the video\n",
    "newWidth, newHeight = calculateWindowSize(scale=0.5, cap=cap)\n",
    "centerY = int(newHeight/2)\n",
    "\n",
    "# line\n",
    "start, end = sv.Point(x=0, y=centerY), sv.Point(x=newWidth, y=centerY)\n",
    "line = sv.LineZone(start=start, end=end, minimum_crossing_threshold=0.5, triggering_anchors=(Position.BOTTOM_LEFT, Position.BOTTOM_RIGHT))\n",
    "lineAnnotator = sv.LineZoneAnnotator(thickness=1, text_scale=0.5, text_thickness=1)\n",
    "\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "while cap.isOpened() : \n",
    "    ret, frame = cap.read()\n",
    "    if not ret :\n",
    "        print(\"Video End\")\n",
    "        exit()\n",
    "\n",
    "    # resize the frame \n",
    "    resizeFrame = cv2.resize(frame, (newWidth, newHeight))\n",
    "    \n",
    "    # detect\n",
    "    result = model(resizeFrame, classes=[0])[0]\n",
    "\n",
    "    # detections\n",
    "    detections = sv.Detections.from_ultralytics(result)\n",
    "\n",
    "    # update \n",
    "    detections = tracker.update_with_detections(detections)\n",
    "\n",
    "    # get in out \n",
    "    goin, goout = line.trigger(detections)\n",
    "    print(f\"goin > {line.in_count}\")\n",
    "    print(f\"goout > {line.out_count}\")\n",
    "\n",
    "    # draw annotated\n",
    "    annotatedFrame = annotated(resizeFrame, detections)\n",
    "\n",
    "    # draw line \n",
    "    annotatedFrame = lineAnnotator.annotate(annotatedFrame.copy(), line_counter=line)\n",
    "\n",
    "    # text\n",
    "    # texts = [f\"Number of Human in frame = {len(detections.tracker_id)}\", f\"Out = {line.out_count}\", f\"In = {line.in_count}\"]\n",
    "    # cv2.putText(img=annotatedFrame, text=texts[0], org=textPosition(width=newWidth, height=newHeight), \n",
    "    #                     fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.5, color=(0,0,0), thickness=1)\n",
    "    # cv2.putText(img=annotatedFrame, text=texts[1], org=textPosition(width=newWidth, height=newHeight+10), \n",
    "    #                     fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.5, color=(0,0,0), thickness=1)\n",
    "    # cv2.putText(img=annotatedFrame, text=texts[2], org=textPosition(width=newWidth, height=newHeight+20), \n",
    "    #                     fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.5, color=(0,0,0), thickness=1)\n",
    "    \n",
    "    cv2.imshow(\"show\", annotatedFrame)\n",
    "    # Exit when 'q' is pressed\n",
    "    if cv2.waitKey(5) & 0xFF == ord('q') :\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdb97ff-2fbd-4081-bc54-e80cf1ee8327",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Trigger Called - In: {line.in_count}, Out: {line.out_count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "migrate",
   "language": "python",
   "name": "migrate"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
